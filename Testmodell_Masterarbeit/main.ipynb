{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masterarbeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhaltsverzeichnis\n",
    "\n",
    "1. Importstatements\n",
    "2. Datenvorbereitung\n",
    "3. Simulationsmodell\n",
    "4. Q-Learning-Agent\n",
    "5. Hyperparameters\n",
    "6. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import multiprocessing\n",
    "import datetime\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "from tensorflow import summary, Variable, Session\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wetterdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weather(path, start, ende):\n",
    "    df = pd.read_csv(\n",
    "        path, \n",
    "        index_col=\"date\", \n",
    "        memory_map=True\n",
    "\n",
    "        )\n",
    "    df = df.drop(columns=[\"Unnamed: 0\", \"HauptGruppe\", \"NebenGruppe\"])\n",
    "    # df = df.sort_index()\n",
    "    # df[\"Datum\"] = df.index.get_values()\n",
    "    # df[\"Datum\"] = pd.to_datetime(df[\"Datum\"]*24*3600, unit='s')\n",
    "    # df = df[df.Datum.dt.year.isin([2018,2019])]\n",
    "    # df = df[df.Datum.dt.dayofweek != 6]\n",
    "    df = df[df.index.isin(range(start, ende +3))]\n",
    "    # Plus 2 Tage, da Wetter von morgen und übermorgen\n",
    "    return df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preisdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prices(path):\n",
    "    df = pd.read_csv(\n",
    "        path, \n",
    "        names=[\"Zeile\", \"Preis\",\"Artikelnummer\",\"Datum\"],\n",
    "        header=0,\n",
    "        index_col=\"Artikelnummer\", \n",
    "        memory_map=True\n",
    "        )\n",
    "    df = df.sort_index()\n",
    "    df = df.drop(columns=[\"Zeile\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absatzdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sales(path):\n",
    "    # TODO: Statische Artikelinfo aus der Absatztabelle rausnehmen. (Warengruppe, Abteilung)\n",
    "    \"\"\"\n",
    "     for artikel in train_data[\"Artikel\"].unique():\n",
    "         warengruppen.append([artikel, train_data.loc[(slice(None), slice(5550,5550)),:].iloc[0].Warengruppe])\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path, \n",
    "        names=[\"Zeile\", \"Datum\", \"Artikel\", \"Absatz\", \"Warengruppe\", \"Abteilung\"], \n",
    "        header=0, \n",
    "        parse_dates=[1], \n",
    "        index_col=[1, 2],\n",
    "        memory_map=True\n",
    "        )\n",
    "    df.dropna(how='any', inplace=True)\n",
    "    df[\"Warengruppe\"] = df[\"Warengruppe\"].astype(np.uint8)\n",
    "    df = df.drop(columns=['Abteilung', 'Zeile'])\n",
    "    # Warengruppen auswählen\n",
    "    # 13 Frischmilch\n",
    "    # 14 Joghurt\n",
    "    # 69 Tabak\n",
    "    # 8 Obst Allgemen\n",
    "    # warengruppen = [8, 13, 14, 69 ]\n",
    "    warengruppen = [8]\n",
    "    df = df[df['Warengruppe'].isin(warengruppen)]\n",
    "    for i, wg in enumerate(warengruppen):\n",
    "        df.loc[df.Warengruppe == wg, \"Warengruppe\"] = i\n",
    "    df[\"Datum\"] = df.index.get_level_values('Datum')\n",
    "    df[\"Artikel\"] = df.index.get_level_values('Artikel').astype(np.int32)\n",
    "    # df[\"Wochentag\"] = df[\"Datum\"].apply(lambda x:x.dayofweek)\n",
    "    # df[\"Jahrestag\"] = df[\"Datum\"].apply(lambda x:x.dayofyear)\n",
    "    df[\"UNIXTag\"] = df[\"Datum\"].astype(np.int64)/(1000000000 * 24 * 3600)\n",
    "    df[\"Jahr\"] = df[\"Datum\"].apply(lambda x:x.year)\n",
    "    # df = df.drop(columns=['Datum'])\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    test_data = df[df[\"Jahr\"]==2019]\n",
    "    train_data = df[df[\"Jahr\"]==2018]\n",
    "    return test_data, train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data_to_numpy(big_df, artikel, start, end):\n",
    "    \"\"\"Returns a numpy array with lenght = self.kalendertage. Days without Sales are filled with zeros\"\"\"\n",
    "    s = big_df[big_df.Artikel == artikel].copy()\n",
    "    s.set_index(s.UNIXTag, inplace=True)\n",
    "    wg = s.iloc[0][[\"Warengruppe\"]][0]\n",
    "    s = s.drop(columns=[\"Datum\", \"Artikel\", \"Warengruppe\", \"Jahr\", \"UNIXTag\"])\n",
    "    s = s.reindex(range(int(start), int(end+1)), fill_value=0)\n",
    "\n",
    "    return s.to_numpy(), wg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulationsmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockSimulation:\n",
    "    def __init__(self, data_dir, time_series_lenght):\n",
    "        \"\"\"\n",
    "        Lädt Daten selbstständig aus Data_dir und erstellt das Simulationsmodell. \n",
    "        1. Episode entspricht einem Durchlauf mit einem Artikel.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        test_data, train_data = load_sales(os.path.join(data_dir, '3 absatz_altforweiler.csv'))\n",
    "\n",
    "        self.df = train_data\n",
    "\n",
    "        self.start_tag = int(min(train_data[\"UNIXTag\"]))\n",
    "        self.end_tag = int(max(train_data[\"UNIXTag\"]))\n",
    "        self.kalender_tage = self.end_tag - self.start_tag + 1\n",
    "\n",
    "        preise = load_prices(os.path.join(data_dir, '3 preise_altforweiler.csv'))\n",
    "\n",
    "        self.wetter = load_weather(os.path.join(data_dir, '2 wetter_saarlouis.csv'), self.start_tag, self.end_tag)\n",
    "        \n",
    "        self.warengruppen = self.df[\"Warengruppe\"].unique()\n",
    "        self.anz_wg = len(self.warengruppen)\n",
    "\n",
    "        self.anfangsbestand = np.random.randint(0,10)\n",
    "\n",
    "        self.time_series_lenght = time_series_lenght\n",
    "\n",
    "        olt = 1  # Fürs erste\n",
    "        self.fertig = None\n",
    "        self.vergangene_tage = None\n",
    "        self.akt_prod_bestand = None\n",
    "        self.akt_prod_absatz = None\n",
    "        self.akt_prod_wg = None\n",
    "        self.akt_prod_preis = None\n",
    "        self.akt_prod_olt = None\n",
    "        self.time_series_state = None\n",
    "\n",
    "        self.absatz_data = {}\n",
    "        self.static_state_data = {}\n",
    "        for artikel in tqdm(self.df[\"Artikel\"].unique()):\n",
    "            art_df, wg = copy_data_to_numpy(self.df, artikel, self.start_tag, self.end_tag)\n",
    "            self.absatz_data[artikel] = art_df\n",
    "            wg = to_categorical(wg, num_classes=self.anz_wg)\n",
    "\n",
    "            artikel_preis = preise.loc[artikel]\n",
    "\n",
    "            if type(artikel_preis) == pd.core.frame.DataFrame:\n",
    "                artikel_preis = np.array(\n",
    "                    [artikel_preis[artikel_preis.Datum == max(artikel_preis.Datum)][\"Preis\"].iat[0]]\n",
    "                )\n",
    "            elif type(artikel_preis) == pd.core.series.Series:\n",
    "                artikel_preis = np.array([artikel_preis[\"Preis\"]])\n",
    "            elif type(artikel_preis) == int:\n",
    "                artikel_preis = np.array([artikel_preis])\n",
    "            else:\n",
    "                raise AssertionError(\"Unknown Type for Price: {}\".format(type(artikel_preis)))\n",
    "            self.static_state_data[artikel] = {\"Warengruppe\":wg, \"OrderLeadTime\": olt, \"Preis\": artikel_preis}\n",
    "\n",
    "        self.aktueller_tag = self.start_tag\n",
    "        self.aktuelles_produkt = self.df[\"Artikel\"].sample(1).to_numpy()[0]\n",
    "\n",
    "    def create_new_state(self, wochentag):\n",
    "        new_state = np.concatenate(\n",
    "            [\n",
    "                np.array([self.akt_prod_bestand]), \n",
    "                wochentag, \n",
    "                self.akt_prod_wg, \n",
    "                self.akt_prod_preis, \n",
    "                self.wetter[self.vergangene_tage], \n",
    "                self.wetter[self.vergangene_tage+1]\n",
    "                ]\n",
    "            )\n",
    "        return new_state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        self.fertig = False\n",
    "        self.anfangsbestand = np.random.randint(0, 10)\n",
    "        self.aktueller_tag = self.start_tag\n",
    "        self.vergangene_tage = 0\n",
    "        self.aktuelles_produkt = self.df[\"Artikel\"].sample(1).to_numpy()[0]\n",
    "        self.akt_prod_bestand = self.anfangsbestand\n",
    "        self.akt_prod_absatz = self.absatz_data[self.aktuelles_produkt]\n",
    "        self.akt_prod_wg = self.static_state_data[self.aktuelles_produkt][\"Warengruppe\"]\n",
    "        self.akt_prod_preis = self.static_state_data[self.aktuelles_produkt][\"Preis\"]\n",
    "        self.akt_prod_olt = self.static_state_data[self.aktuelles_produkt][\"OrderLeadTime\"]\n",
    "\n",
    "        wochentag = self.aktueller_tag % 7\n",
    "\n",
    "        wochentag = to_categorical(wochentag, num_classes=7)\n",
    "\n",
    "        new_state = self.create_new_state(wochentag)\n",
    "        \n",
    "        self.time_series_state = deque(maxlen=self.time_series_lenght)\n",
    "        for _ in range(self.time_series_lenght):\n",
    "            self.time_series_state.append(new_state)\n",
    "        return np.array(self.time_series_state), {\"Artikel\": self.aktuelles_produkt}\n",
    "\n",
    "    def make_action(self, action):\n",
    "        if self.fertig:\n",
    "            raise AssertionError(\"Simulation für diesen Artikel fertig. Simulation zurücksetzen\")\n",
    "\n",
    "        absatz = self.akt_prod_absatz[self.vergangene_tage][0]\n",
    "\n",
    "        self.aktueller_tag += 1\n",
    "        self.vergangene_tage += 1\n",
    "\n",
    "        if self.aktueller_tag % 7 == 3: # Sonntag\n",
    "            self.aktueller_tag += 1\n",
    "            self.vergangene_tage += 1\n",
    "        \n",
    "        wochentag = self.aktueller_tag % 7\n",
    "\n",
    "        # Action ist die Bestellte Menge an Artikeln\n",
    "        # Tagsüber Absatz abziehen:\n",
    "        self.akt_prod_bestand -= absatz\n",
    "\n",
    "        # Nachmittag: Bestellung kommt an\n",
    "        self.akt_prod_bestand += action\n",
    "\n",
    "        # Abend: Bestand wird bewertet\n",
    "        if self.akt_prod_bestand >= 1:\n",
    "            reward = np.exp((-self.akt_prod_bestand+1)/5)\n",
    "        else:\n",
    "            reward = np.exp((self.akt_prod_bestand-1)*1.5-1)\n",
    "            # Nichtnegativität des Bestandes\n",
    "            self.akt_prod_bestand = 0\n",
    "\n",
    "        wochentag = to_categorical(wochentag, num_classes=7)\n",
    "        \n",
    "        new_state = self.create_new_state(wochentag)\n",
    "\n",
    "        self.time_series_state.append(new_state)\n",
    "\n",
    "        if self.vergangene_tage == self.kalender_tage -1:\n",
    "            self.fertig = True\n",
    "        \n",
    "        return reward, self.fertig, np.array(self.time_series_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, \n",
    "                 memory_size, \n",
    "                 state_shape, \n",
    "                 action_space, \n",
    "                 gamma, \n",
    "                 learning_rate, \n",
    "                 batch_size, \n",
    "                 epsilon, \n",
    "                 epsilon_decay, \n",
    "                 epsilon_min, \n",
    "                 possible_actions, \n",
    "                 time_series_length\n",
    "                 ):\n",
    "        self.memory_size = memory_size\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon \n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.possible_actions = possible_actions\n",
    "        self.time_series_length = time_series_length\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.model = self.create_model()\n",
    "        self.logdir = \"./logs/\" + datetime.datetime.today().date().__str__() + \"-\" \\\n",
    "                      + datetime.datetime.today().time().__str__()[:8].replace(\":\", \".\")\n",
    "        self.target_model = self.create_model()\n",
    "        self.sess = Session()\n",
    "        self.writer = summary.FileWriter(self.logdir, self.sess.graph)\n",
    "        self.reward = Variable(0.0, trainable=False, name=\"vReward\")\n",
    "        self.reward_mean = Variable(0.0, trainable=False, name=\"vMeanReward\")\n",
    "        self.loss = Variable(0.0, trainable=False, name=\"vLoss\")\n",
    "        self.accuracy = Variable(0.0, trainable=False, name=\"vMSE\")\n",
    "        self.summary_reward = summary.scalar(\"Reward\", self.reward)\n",
    "        self.summary_reward_mean = summary.scalar(\"MeanReward\", self.reward_mean)\n",
    "        self.summary_loss = summary.scalar(\"Loss\", self.loss)\n",
    "        self.summary_mse = summary.scalar(\"Accuracy\", self.accuracy)\n",
    "        self.merged = summary.merge(\n",
    "            [\n",
    "                self.summary_reward, \n",
    "                self.summary_reward_mean, \n",
    "                self.summary_loss, \n",
    "                self.summary_mse\n",
    "            ])\n",
    "\n",
    "    def create_model(self):\n",
    "        inputs = Input(shape=(self.time_series_length, self.state_shape))\n",
    "        x = LSTM(32, activation='relu')(inputs)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        predictions = Dense(self.action_space, activation='relu')(x)\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(optimizer=RMSprop(lr=self.learning_rate), loss='mse', metrics=[\"accuracy\"])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = [sample[0] for sample in samples]\n",
    "        actions = [sample[1] for sample in samples]\n",
    "        rewards = [sample[2] for sample in samples]\n",
    "        new_states = [sample[3] for sample in samples]\n",
    "        new_states = np.array(new_states)\n",
    "        states = np.array(states)\n",
    "        dones = [sample[4] for sample in samples]\n",
    "        targets = self.target_model.predict(states)\n",
    "        qs_new_states = self.target_model.predict(new_states)\n",
    "        \n",
    "        target_qs_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            terminal = dones[i]\n",
    "\n",
    "            if terminal:\n",
    "                updated_target = targets[i]\n",
    "                updated_target[actions[i]] = rewards[i]\n",
    "                target_qs_batch.append(updated_target)\n",
    "            else:\n",
    "                updated_target = targets[i]\n",
    "                updated_target[actions[i]] = rewards[i] + self.gamma * np.max(qs_new_states[i])\n",
    "                target_qs_batch.append(updated_target)\n",
    "\n",
    "        targets = np.array([each for each in target_qs_batch])\n",
    "\n",
    "        history = self.model.fit(states, targets, epochs=1, verbose=0, callbacks=[])\n",
    "        return history.history\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i]\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = np.max([self.epsilon, self.epsilon_min])\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.sample(self.possible_actions, 1)[0]\n",
    "        return np.argmax(self.model.predict(state.reshape(1, self.time_series_length, self.state_shape))[0])\n",
    "    \n",
    "    def save(self):\n",
    "        agent.target_model.save(\"model/model.h5\")\n",
    "    \n",
    "    def load(self):\n",
    "        model = load_model(\"model/model.h5\")\n",
    "        agent.target_model = model\n",
    "        agent.model = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = True\n",
    "\n",
    "use_saved_model = False\n",
    "\n",
    "\n",
    "memory_size = 364*500\n",
    "gamma = 0.5\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99999\n",
    "learning_rate = 0.001\n",
    "tau = 0.05\n",
    "batch_size = 128\n",
    "n_step = 300\n",
    "log_frequency = 100  # jeder 100te n_step\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "update_target_network = batch_size * 100\n",
    "\n",
    "state_shape = 24\n",
    "action_space = 10\n",
    "\n",
    "time_series_length = 10\n",
    "\n",
    "order_none = 0\n",
    "order_one = 1\n",
    "order_two = 2\n",
    "order_tree = 3\n",
    "order_four = 4\n",
    "order_five = 5\n",
    "order_six = 6\n",
    "order_seven = 7\n",
    "order_eight = 8\n",
    "order_nine = 9\n",
    "\n",
    "possible_actions = [\n",
    "    order_none, \n",
    "    order_one, \n",
    "    order_two, \n",
    "    order_tree, \n",
    "    order_four, \n",
    "    order_five, \n",
    "    order_six, \n",
    "    order_seven, \n",
    "    order_eight, \n",
    "    order_nine\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsloop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.environ[\"OS\"] == \"Windows_NT\"\n",
    "    # Bin am eigenen Desktop\n",
    "    data_dir = 'F:/OneDrive/Dokumente/1 Universität - Master/6. Semester/Masterarbeit/Implementation/Echtdaten'\n",
    "except KeyError:\n",
    "    # Bin auf der EC2 Linux Maschine \n",
    "    data_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "100%|██████████| 566/566 [00:03<00:00, 188.66it/s]\n"
     ]
    }
   ],
   "source": [
    "simulation = StockSimulation(data_dir, time_series_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "agent = DQN(\n",
    "    memory_size, \n",
    "    state_shape, \n",
    "    action_space, \n",
    "    gamma,\n",
    "    learning_rate, \n",
    "    batch_size, \n",
    "    epsilon, \n",
    "    epsilon_decay, \n",
    "    epsilon_min, \n",
    "    possible_actions, \n",
    "    time_series_length\n",
    "    )\n",
    "\n",
    "if use_saved_model:\n",
    "    agent.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 0\n",
      "\tMean reward: 0.0008779494955179615 --- Total Reward: 0.273042293106086 --- EXP-EXP: 0.9968948155387197\n",
      "Epoch 10\n",
      "\tMean reward: 0.0023456116688027598 --- Total Reward: 0.7294852289976583 --- EXP-EXP: 0.9663683806346631\n",
      "Epoch 20\n",
      "\tMean reward: 0.0004982307282028126 --- Total Reward: 0.15494975647107473 --- EXP-EXP: 0.9367767115789445\n",
      "Epoch 30\n",
      "\tMean reward: 0.0009730354791500945 --- Total Reward: 0.3026140340156794 --- EXP-EXP: 0.9080911844200968\n",
      "Epoch 40\n",
      "\tMean reward: 0.0002833072845165684 --- Total Reward: 0.08810856548465278 --- EXP-EXP: 0.8802840517155684\n",
      "Epoch 50\n",
      "\tMean reward: 0.001437969463922863 --- Total Reward: 0.4472085032800104 --- EXP-EXP: 0.8533284156916687\n",
      "Epoch 60\n",
      "\tMean reward: 0.004156401558316673 --- Total Reward: 1.2926408846364854 --- EXP-EXP: 0.8271982022254514\n",
      "Epoch 70\n",
      "\tMean reward: 0.0039489630625457635 --- Total Reward: 1.2281275124517324 --- EXP-EXP: 0.8018681356232514\n",
      "Epoch 80\n",
      "\tMean reward: 0.0018507937762915873 --- Total Reward: 0.5755968644266837 --- EXP-EXP: 0.7773137141715685\n",
      "Epoch 90\n",
      "\tMean reward: 0.0011309583259679697 --- Total Reward: 0.3517280393760386 --- EXP-EXP: 0.7535111864366185\n",
      "Epoch 100\n",
      "\tMean reward: 0.0010515539931578017 --- Total Reward: 0.3270332918720763 --- EXP-EXP: 0.7304375282896425\n",
      "Epoch 110\n",
      "\tMean reward: 0.1305370475998126 --- Total Reward: 40.59702180354172 --- EXP-EXP: 0.7080704206356979\n",
      "Epoch 120\n",
      "\tMean reward: 0.0003076425224637677 --- Total Reward: 0.09567682448623174 --- EXP-EXP: 0.6863882278244717\n",
      "Epoch 130\n",
      "\tMean reward: 0.04126010791650492 --- Total Reward: 12.831893562033029 --- EXP-EXP: 0.6653699767221523\n",
      "Epoch 140\n",
      "\tMean reward: 0.0009244443726677068 --- Total Reward: 0.2875021998996568 --- EXP-EXP: 0.6449953364241736\n",
      "Epoch 150\n",
      "\tMean reward: 0.0012417670234477467 --- Total Reward: 0.3861895442922492 --- EXP-EXP: 0.6252445985891766\n",
      "Epoch 160\n",
      "\tMean reward: 0.10982900584158464 --- Total Reward: 34.156820816732825 --- EXP-EXP: 0.6060986583751835\n",
      "Epoch 170\n",
      "\tMean reward: 0.002051084840853694 --- Total Reward: 0.6378873855054987 --- EXP-EXP: 0.5875389959595214\n",
      "Epoch 180\n",
      "\tMean reward: 0.0010419635421609662 --- Total Reward: 0.3240506616120605 --- EXP-EXP: 0.5695476586246405\n",
      "Epoch 190\n",
      "\tMean reward: 0.00045532595124315507 --- Total Reward: 0.14160637083662123 --- EXP-EXP: 0.5521072433925005\n",
      "Epoch 200\n",
      "\tMean reward: 0.006526385708710318 --- Total Reward: 2.0297059554089087 --- EXP-EXP: 0.5352008801907125\n",
      "Epoch 210\n",
      "\tMean reward: 0.00038855493845334 --- Total Reward: 0.12084058585898874 --- EXP-EXP: 0.5188122155341464\n",
      "Epoch 220\n",
      "\tMean reward: 0.0007064918639744055 --- Total Reward: 0.21971896969604013 --- EXP-EXP: 0.5029253967062505\n",
      "Epoch 230\n",
      "\tMean reward: 0.002712882573252919 --- Total Reward: 0.8437064802816577 --- EXP-EXP: 0.4875250564247611\n",
      "Epoch 240\n",
      "\tMean reward: 0.0009404011286322488 --- Total Reward: 0.2924647510046294 --- EXP-EXP: 0.47259629797695585\n",
      "Epoch 250\n",
      "\tMean reward: 0.0004040862214768029 --- Total Reward: 0.1256708148792857 --- EXP-EXP: 0.45812468081011043\n",
      "Epoch 260\n",
      "\tMean reward: 0.002379686573163004 --- Total Reward: 0.7400825242536941 --- EXP-EXP: 0.44409620656317295\n",
      "Epoch 270\n",
      "\tMean reward: 0.0007569873604122452 --- Total Reward: 0.23542306908820823 --- EXP-EXP: 0.43049730552619336\n",
      "Epoch 280\n",
      "\tMean reward: 0.0005782010197289839 --- Total Reward: 0.179820517135714 --- EXP-EXP: 0.4173148235143741\n",
      "Epoch 290\n",
      "\tMean reward: 0.00020360619059673273 --- Total Reward: 0.06332152527558388 --- EXP-EXP: 0.40453600914404775\n",
      "Epoch 300\n",
      "\tMean reward: 0.0005349924737236482 --- Total Reward: 0.16638265932805457 --- EXP-EXP: 0.3921485014983119\n",
      "Epoch 310\n",
      "\tMean reward: 0.00027642082737881703 --- Total Reward: 0.08596687731481209 --- EXP-EXP: 0.38014031817032307\n",
      "Epoch 320\n",
      "\tMean reward: 0.0012713408649496444 --- Total Reward: 0.39538700899933943 --- EXP-EXP: 0.36849984367275807\n",
      "Epoch 330\n",
      "\tMean reward: 0.0021530912156420973 --- Total Reward: 0.6696113680646922 --- EXP-EXP: 0.3572158182021752\n",
      "Epoch 340\n",
      "\tMean reward: 0.00023138656161649727 --- Total Reward: 0.07196122066273065 --- EXP-EXP: 0.3462773267474337\n",
      "Epoch 350\n",
      "\tMean reward: 0.004391045284876565 --- Total Reward: 1.3656150835966117 --- EXP-EXP: 0.3356737885316259\n",
      "Epoch 360\n",
      "\tMean reward: 0.00022287611269250615 --- Total Reward: 0.06931447104736942 --- EXP-EXP: 0.3253949467773238\n",
      "Epoch 370\n",
      "\tMean reward: 0.001831977772987655 --- Total Reward: 0.5697450873991607 --- EXP-EXP: 0.3154308587852121\n",
      "Epoch 380\n",
      "\tMean reward: 0.0003698694774842411 --- Total Reward: 0.11502940749759898 --- EXP-EXP: 0.3057718863165531\n",
      "Epoch 390\n",
      "\tMean reward: 0.0003822094063810441 --- Total Reward: 0.11886712538450472 --- EXP-EXP: 0.29640868627012895\n",
      "Epoch 400\n",
      "\tMean reward: 0.0010519789615094624 --- Total Reward: 0.3271654570294428 --- EXP-EXP: 0.28733220164468537\n",
      "Epoch 410\n",
      "\tMean reward: 0.00031670595944678734 --- Total Reward: 0.09849555338795087 --- EXP-EXP: 0.27853365277811726\n",
      "Epoch 420\n",
      "\tMean reward: 0.00045157386502753445 --- Total Reward: 0.14043947202356322 --- EXP-EXP: 0.2700045288549214\n",
      "Epoch 430\n",
      "\tMean reward: 0.0004713391072173628 --- Total Reward: 0.14658646234459982 --- EXP-EXP: 0.2617365796737083\n",
      "Epoch 440\n",
      "\tMean reward: 0.00039357216661973187 --- Total Reward: 0.1224009438187366 --- EXP-EXP: 0.25372180766679314\n",
      "Epoch 450\n",
      "\tMean reward: 0.009482066211781203 --- Total Reward: 2.948922591863954 --- EXP-EXP: 0.2459524601641747\n",
      "Epoch 460\n",
      "\tMean reward: 0.001108674991429927 --- Total Reward: 0.34479792233470724 --- EXP-EXP: 0.23842102189439446\n",
      "Epoch 470\n",
      "\tMean reward: 0.0023386544970939787 --- Total Reward: 0.7273215485962273 --- EXP-EXP: 0.2311202077150321\n",
      "Epoch 480\n",
      "\tMean reward: 0.0019163538583575833 --- Total Reward: 0.5959860499492085 --- EXP-EXP: 0.22404295556580517\n",
      "Epoch 490\n",
      "\tMean reward: 0.0010380298619487199 --- Total Reward: 0.3228272870660519 --- EXP-EXP: 0.2171824196374532\n",
      "Epoch 500\n",
      "\tMean reward: 0.0002878476328872709 --- Total Reward: 0.08952061382794124 --- EXP-EXP: 0.21053196374980296\n",
      "Epoch 510\n",
      "\tMean reward: 0.07865288745725536 --- Total Reward: 24.461047999206418 --- EXP-EXP: 0.20408515493260798\n",
      "Epoch 520\n",
      "\tMean reward: 0.0002451081605057759 --- Total Reward: 0.0762286379172963 --- EXP-EXP: 0.19783575720295116\n",
      "Epoch 530\n",
      "\tMean reward: 0.0046403900229938615 --- Total Reward: 1.443161297151091 --- EXP-EXP: 0.1917777255331938\n",
      "Epoch 540\n",
      "\tMean reward: 0.00031288347203715184 --- Total Reward: 0.09730675980355423 --- EXP-EXP: 0.18590520000363367\n",
      "Epoch 550\n",
      "\tMean reward: 0.0002559458170481078 --- Total Reward: 0.07959914910196153 --- EXP-EXP: 0.18021250013422083\n",
      "Epoch 560\n",
      "\tMean reward: 0.0012836209885401307 --- Total Reward: 0.39920612743598066 --- EXP-EXP: 0.17469411938983725\n",
      "Epoch 570\n",
      "\tMean reward: 0.0003818276195482092 --- Total Reward: 0.11874838967949305 --- EXP-EXP: 0.1693447198538457\n",
      "Epoch 580\n",
      "\tMean reward: 0.08120767913479768 --- Total Reward: 25.255588210922078 --- EXP-EXP: 0.1641591270647305\n",
      "Epoch 590\n",
      "\tMean reward: 0.0003504670475604749 --- Total Reward: 0.10899525179130769 --- EXP-EXP: 0.1591323250108549\n",
      "Epoch 600\n",
      "\tMean reward: 0.0007212639629003928 --- Total Reward: 0.22431309246202216 --- EXP-EXP: 0.1542594512784846\n",
      "Epoch 610\n",
      "\tMean reward: 0.000254558594668864 --- Total Reward: 0.07916772294201671 --- EXP-EXP: 0.14953579234838626\n",
      "Epoch 620\n",
      "\tMean reward: 0.0006255590258320946 --- Total Reward: 0.19454885703378144 --- EXP-EXP: 0.14495677903645313\n",
      "Epoch 630\n",
      "\tMean reward: 0.00020955134742735282 --- Total Reward: 0.06517046904990673 --- EXP-EXP: 0.1405179820739409\n",
      "Epoch 640\n",
      "\tMean reward: 0.0024799525061797627 --- Total Reward: 0.7712652294219062 --- EXP-EXP: 0.1362151078230493\n",
      "Epoch 650\n",
      "\tMean reward: 0.0004814720553618016 --- Total Reward: 0.1497378092175203 --- EXP-EXP: 0.1320439941236948\n",
      "Epoch 660\n",
      "\tMean reward: 0.0031374946792933492 --- Total Reward: 0.9757608452602317 --- EXP-EXP: 0.12800060626746498\n",
      "Epoch 670\n",
      "\tMean reward: 0.0032153572087499013 --- Total Reward: 0.9999760919212193 --- EXP-EXP: 0.12408103309485063\n",
      "Epoch 680\n",
      "\tMean reward: 0.003086881680168551 --- Total Reward: 0.9600202025324194 --- EXP-EXP: 0.12028148321199687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 690\n",
      "\tMean reward: 0.0006602619083619587 --- Total Reward: 0.20534145350056915 --- EXP-EXP: 0.1165982813232902\n",
      "Epoch 700\n",
      "\tMean reward: 0.004973215603145374 --- Total Reward: 1.5466700525782113 --- EXP-EXP: 0.1130278646762574\n",
      "Epoch 710\n",
      "\tMean reward: 0.06790791762424409 --- Total Reward: 21.11936238113991 --- EXP-EXP: 0.10956677961532287\n",
      "Epoch 720\n",
      "\tMean reward: 0.0006287981258414761 --- Total Reward: 0.19555621713669907 --- EXP-EXP: 0.10621167824109574\n",
      "Epoch 730\n",
      "\tMean reward: 0.0008573713941310737 --- Total Reward: 0.26664250357476393 --- EXP-EXP: 0.10295931517195399\n",
      "Epoch 740\n",
      "\tMean reward: 0.007871798045699659 --- Total Reward: 2.4481291922125936 --- EXP-EXP: 0.09980654440479556\n",
      "Epoch 750\n",
      "\tMean reward: 0.00062068581979014 --- Total Reward: 0.19303328995473354 --- EXP-EXP: 0.09675031627191585\n",
      "Epoch 760\n",
      "\tMean reward: 0.0033381301148562425 --- Total Reward: 1.0381584657202914 --- EXP-EXP: 0.093787674491072\n",
      "Epoch 770\n",
      "\tMean reward: 0.11850922195200295 --- Total Reward: 36.85636802707292 --- EXP-EXP: 0.09091575330587927\n",
      "Epoch 780\n",
      "\tMean reward: 0.0027866728063560553 --- Total Reward: 0.8666552427767332 --- EXP-EXP: 0.08813177471376925\n",
      "Epoch 790\n",
      "\tMean reward: 0.0027821830630995726 --- Total Reward: 0.8652589326239671 --- EXP-EXP: 0.08543304577883674\n",
      "Epoch 800\n",
      "\tMean reward: 0.0047280933985070946 --- Total Reward: 1.4704370469357064 --- EXP-EXP: 0.08281695602696733\n",
      "Epoch 810\n",
      "\tMean reward: 0.004426043607466046 --- Total Reward: 1.3764995619219405 --- EXP-EXP: 0.08028097492073269\n",
      "Epoch 820\n",
      "\tMean reward: 0.001248320706054117 --- Total Reward: 0.38822773958283036 --- EXP-EXP: 0.0778226494116094\n",
      "Epoch 830\n",
      "\tMean reward: 0.0033922341423781094 --- Total Reward: 1.054984818279592 --- EXP-EXP: 0.07543960156714778\n",
      "Epoch 840\n",
      "\tMean reward: 0.0020717260169529623 --- Total Reward: 0.6443067912723712 --- EXP-EXP: 0.07312952627080581\n",
      "Epoch 850\n",
      "\tMean reward: 0.006123138884631616 --- Total Reward: 1.9042961931204327 --- EXP-EXP: 0.07089018899221496\n",
      "Epoch 860\n",
      "\tMean reward: 0.0033749710188618783 --- Total Reward: 1.0496159868660442 --- EXP-EXP: 0.06871942362572314\n",
      "Epoch 870\n",
      "\tMean reward: 0.004366686641566841 --- Total Reward: 1.3580395455272876 --- EXP-EXP: 0.06661513039512673\n",
      "Epoch 880\n",
      "\tMean reward: 0.002169263199890287 --- Total Reward: 0.6746408551658792 --- EXP-EXP: 0.06457527382256237\n",
      "Epoch 890\n",
      "\tMean reward: 0.00503938782612426 --- Total Reward: 1.567249613924645 --- EXP-EXP: 0.06259788075959331\n",
      "Epoch 900\n",
      "\tMean reward: 0.00478868084945769 --- Total Reward: 1.4892797441813415 --- EXP-EXP: 0.060681038478588\n",
      "Epoch 910\n",
      "\tMean reward: 0.00403858199721011 --- Total Reward: 1.255999001132344 --- EXP-EXP: 0.058822892822542834\n",
      "Epoch 920\n",
      "\tMean reward: 0.007986417612711362 --- Total Reward: 2.4837758775532333 --- EXP-EXP: 0.057021646411561176\n",
      "Epoch 930\n",
      "\tMean reward: 0.004436369496095949 --- Total Reward: 1.37971091328584 --- EXP-EXP: 0.05527555690425061\n",
      "Epoch 940\n",
      "\tMean reward: 0.0028340081286328867 --- Total Reward: 0.8813765280048278 --- EXP-EXP: 0.053582935312361635\n",
      "Epoch 950\n",
      "\tMean reward: 0.008200641428042643 --- Total Reward: 2.550399484121262 --- EXP-EXP: 0.051942144367033156\n",
      "Epoch 960\n",
      "\tMean reward: 0.006691362394370267 --- Total Reward: 2.081013704649153 --- EXP-EXP: 0.0503515969350652\n",
      "Epoch 970\n",
      "\tMean reward: 0.014517269808317667 --- Total Reward: 4.5148709103867946 --- EXP-EXP: 0.048809754483689856\n",
      "Epoch 980\n",
      "\tMean reward: 0.010354866516645456 --- Total Reward: 3.220363486676737 --- EXP-EXP: 0.0473151255923518\n",
      "Epoch 990\n",
      "\tMean reward: 0.004783794894251791 --- Total Reward: 1.4877602121123068 --- EXP-EXP: 0.04586626451006034\n",
      "Epoch 1000\n",
      "\tMean reward: 0.1492501463209616 --- Total Reward: 46.41679550581905 --- EXP-EXP: 0.04446176975691847\n",
      "Epoch 1010\n",
      "\tMean reward: 0.007531568386240217 --- Total Reward: 2.3423177681207075 --- EXP-EXP: 0.04310028276847403\n",
      "Epoch 1020\n",
      "\tMean reward: 0.008007316602521318 --- Total Reward: 2.4902754633841298 --- EXP-EXP: 0.04178048658158432\n",
      "Epoch 1030\n",
      "\tMean reward: 0.017957550109151198 --- Total Reward: 5.584798083946023 --- EXP-EXP: 0.04050110456052012\n",
      "Epoch 1040\n",
      "\tMean reward: 0.006710374403264564 --- Total Reward: 2.0869264394152793 --- EXP-EXP: 0.039260899162079047\n",
      "Epoch 1050\n",
      "\tMean reward: 0.01047353070148971 --- Total Reward: 3.2572680481633 --- EXP-EXP: 0.03805867073851354\n",
      "Epoch 1060\n",
      "\tMean reward: 0.010005739414090898 --- Total Reward: 3.111784957782269 --- EXP-EXP: 0.03689325637711336\n",
      "Epoch 1070\n",
      "\tMean reward: 0.0012855848468297086 --- Total Reward: 0.39981688736403936 --- EXP-EXP: 0.03576352877532397\n",
      "Epoch 1080\n",
      "\tMean reward: 0.01916826746389998 --- Total Reward: 5.961331181272894 --- EXP-EXP: 0.03466839515030926\n",
      "Epoch 1090\n",
      "\tMean reward: 0.02222045855797831 --- Total Reward: 6.910562611531255 --- EXP-EXP: 0.03360679618190469\n",
      "Epoch 1100\n",
      "\tMean reward: 0.029700548580507163 --- Total Reward: 9.236870608537728 --- EXP-EXP: 0.03257770498793939\n",
      "Epoch 1110\n",
      "\tMean reward: 0.017715111069232313 --- Total Reward: 5.509399542531249 --- EXP-EXP: 0.03158012613093619\n",
      "Epoch 1120\n",
      "\tMean reward: 0.019513640243182334 --- Total Reward: 6.068742115629706 --- EXP-EXP: 0.030613094655226335\n",
      "Epoch 1130\n",
      "\tMean reward: 0.14356057947382814 --- Total Reward: 44.64734021636055 --- EXP-EXP: 0.02967567515355164\n",
      "Epoch 1140\n",
      "\tMean reward: 0.041612755165510336 --- Total Reward: 12.941566856473715 --- EXP-EXP: 0.028766960862245866\n",
      "Epoch 1150\n",
      "\tMean reward: 0.09651789291915473 --- Total Reward: 30.017064697857123 --- EXP-EXP: 0.027886072784124687\n",
      "Epoch 1160\n",
      "\tMean reward: 0.017461916755868317 --- Total Reward: 5.430656111075047 --- EXP-EXP: 0.027032158838234212\n",
      "Epoch 1170\n",
      "\tMean reward: 0.0065777126461801045 --- Total Reward: 2.0456686329620126 --- EXP-EXP: 0.02620439303563499\n",
      "Epoch 1180\n",
      "\tMean reward: 0.0694519203008727 --- Total Reward: 21.599547213571412 --- EXP-EXP: 0.025401974680424457\n",
      "Epoch 1190\n",
      "\tMean reward: 0.025732997140827017 --- Total Reward: 8.002962110797203 --- EXP-EXP: 0.02462412759522584\n",
      "Epoch 1200\n",
      "\tMean reward: 0.08455230732939711 --- Total Reward: 26.2957675794425 --- EXP-EXP: 0.023870099370393978\n",
      "Epoch 1210\n",
      "\tMean reward: 0.05275196645078182 --- Total Reward: 16.405861566193146 --- EXP-EXP: 0.02313916063621077\n",
      "Epoch 1220\n",
      "\tMean reward: 0.03309276503796132 --- Total Reward: 10.29184992680597 --- EXP-EXP: 0.022430604357368072\n",
      "Epoch 1230\n",
      "\tMean reward: 0.04991072310783259 --- Total Reward: 15.522234886535935 --- EXP-EXP: 0.021743745149053653\n",
      "Epoch 1240\n",
      "\tMean reward: 0.17757863654513215 --- Total Reward: 55.2269559655361 --- EXP-EXP: 0.021077918613980208\n",
      "Epoch 1250\n",
      "\tMean reward: 0.004088596697192255 --- Total Reward: 1.2715535728267913 --- EXP-EXP: 0.02043248069971547\n",
      "Epoch 1260\n",
      "\tMean reward: 0.15151445327209154 --- Total Reward: 47.12099496762047 --- EXP-EXP: 0.01980680707569205\n",
      "Epoch 1270\n",
      "\tMean reward: 0.08027402480854869 --- Total Reward: 24.96522171545864 --- EXP-EXP: 0.019200292529293644\n",
      "Epoch 1280\n",
      "\tMean reward: 0.08579912163422711 --- Total Reward: 26.68352682824463 --- EXP-EXP: 0.018612350380434642\n",
      "Epoch 1290\n",
      "\tMean reward: 0.1370696552811609 --- Total Reward: 42.62866279244104 --- EXP-EXP: 0.01804241191406519\n",
      "Epoch 1300\n",
      "\tMean reward: 0.007525270761083405 --- Total Reward: 2.340359206696939 --- EXP-EXP: 0.0174899258300552\n",
      "Epoch 1310\n",
      "\tMean reward: 0.020555637294072747 --- Total Reward: 6.392803198456624 --- EXP-EXP: 0.016954357709922818\n",
      "Epoch 1320\n",
      "\tMean reward: 0.14665344061986013 --- Total Reward: 45.6092200327765 --- EXP-EXP: 0.016435189499892224\n",
      "Epoch 1330\n",
      "\tMean reward: 0.09989123756171073 --- Total Reward: 31.066174881692035 --- EXP-EXP: 0.01593191900978225\n",
      "Epoch 1340\n",
      "\tMean reward: 0.11059108640977781 --- Total Reward: 34.3938278734409 --- EXP-EXP: 0.015444059427238477\n",
      "Epoch 1350\n",
      "\tMean reward: 0.1343440969763477 --- Total Reward: 41.78101415964414 --- EXP-EXP: 0.014971138846840868\n",
      "Epoch 1360\n",
      "\tMean reward: 0.1365209427613576 --- Total Reward: 42.45801319878221 --- EXP-EXP: 0.014512699813630847\n",
      "Epoch 1370\n",
      "\tMean reward: 0.15739543892463573 --- Total Reward: 48.94998150556171 --- EXP-EXP: 0.014068298880616215\n",
      "Epoch 1380\n",
      "\tMean reward: 0.15116488868760733 --- Total Reward: 47.012280381845876 --- EXP-EXP: 0.013637506179825738\n",
      "Epoch 1390\n",
      "\tMean reward: 0.17828403032416218 --- Total Reward: 55.446333430814434 --- EXP-EXP: 0.013219905006499251\n",
      "Epoch 1400\n",
      "\tMean reward: 0.22821298878860202 --- Total Reward: 70.97423951325523 --- EXP-EXP: 0.012815091416009672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1410\n",
      "\tMean reward: 0.051183098545440855 --- Total Reward: 15.917943647632105 --- EXP-EXP: 0.01242267383312865\n",
      "Epoch 1420\n",
      "\tMean reward: 0.030173484839858204 --- Total Reward: 9.383953785195901 --- EXP-EXP: 0.012042272673256684\n",
      "Epoch 1430\n",
      "\tMean reward: 0.1420352655536554 --- Total Reward: 44.172967587186825 --- EXP-EXP: 0.011673519975251732\n",
      "Epoch 1440\n",
      "\tMean reward: 0.0761225467086146 --- Total Reward: 23.674112026379138 --- EXP-EXP: 0.011316059045501467\n",
      "Epoch 1450\n",
      "\tMean reward: 0.054959492867378154 --- Total Reward: 17.092402281754605 --- EXP-EXP: 0.010969544112894174\n",
      "Epoch 1460\n",
      "\tMean reward: 0.13773805708736195 --- Total Reward: 42.83653575416957 --- EXP-EXP: 0.010633639994355395\n",
      "Epoch 1470\n",
      "\tMean reward: 0.1007026888232828 --- Total Reward: 31.31853622404095 --- EXP-EXP: 0.010308021770625902\n",
      "Epoch 1480\n",
      "\tMean reward: 0.0036049748100821684 --- Total Reward: 1.1211471659355543 --- EXP-EXP: 0.01\n",
      "Epoch 1490\n",
      "\tMean reward: 0.07377998203181663 --- Total Reward: 22.94557441189497 --- EXP-EXP: 0.01\n",
      "Epoch 1500\n",
      "\tMean reward: 0.13206798431085112 --- Total Reward: 41.0731431206747 --- EXP-EXP: 0.01\n",
      "Epoch 1510\n",
      "\tMean reward: 0.038170358252719264 --- Total Reward: 11.87098141659569 --- EXP-EXP: 0.01\n",
      "Epoch 1520\n",
      "\tMean reward: 0.12734469556560807 --- Total Reward: 39.60420032090411 --- EXP-EXP: 0.01\n",
      "Epoch 1530\n",
      "\tMean reward: 0.04853792518517149 --- Total Reward: 15.095294732588334 --- EXP-EXP: 0.01\n",
      "Epoch 1540\n",
      "\tMean reward: 0.011663289106240359 --- Total Reward: 3.6272829120407515 --- EXP-EXP: 0.01\n",
      "Epoch 1550\n",
      "\tMean reward: 0.1359037244227013 --- Total Reward: 42.2660582954601 --- EXP-EXP: 0.01\n",
      "Epoch 1560\n",
      "\tMean reward: 0.038677312484364026 --- Total Reward: 12.028644182637212 --- EXP-EXP: 0.01\n",
      "Epoch 1570\n",
      "\tMean reward: 0.2800367017963047 --- Total Reward: 87.09141425865076 --- EXP-EXP: 0.01\n",
      "Epoch 1580\n",
      "\tMean reward: 0.06598612586530622 --- Total Reward: 20.521685144110236 --- EXP-EXP: 0.01\n",
      "Epoch 1590\n",
      "\tMean reward: 0.3959941844100572 --- Total Reward: 123.15419135152779 --- EXP-EXP: 0.01\n",
      "Epoch 1600\n",
      "\tMean reward: 0.36267251958728575 --- Total Reward: 112.79115359164587 --- EXP-EXP: 0.01\n",
      "Epoch 1610\n",
      "\tMean reward: 0.02542724676537995 --- Total Reward: 7.907873744033164 --- EXP-EXP: 0.01\n",
      "Epoch 1620\n",
      "\tMean reward: 0.2401559386120417 --- Total Reward: 74.68849690834497 --- EXP-EXP: 0.01\n",
      "Epoch 1630\n",
      "\tMean reward: 0.23560148861366204 --- Total Reward: 73.27206295884889 --- EXP-EXP: 0.01\n",
      "Epoch 1640\n",
      "\tMean reward: 0.4125074871177772 --- Total Reward: 128.2898284936287 --- EXP-EXP: 0.01\n",
      "Epoch 1650\n",
      "\tMean reward: 0.24142431344685258 --- Total Reward: 75.08296148197115 --- EXP-EXP: 0.01\n",
      "Epoch 1660\n",
      "\tMean reward: 0.08008175582777055 --- Total Reward: 24.90542606243664 --- EXP-EXP: 0.01\n",
      "Epoch 1670\n",
      "\tMean reward: 0.2762049393661066 --- Total Reward: 85.89973614285915 --- EXP-EXP: 0.01\n",
      "Epoch 1680\n",
      "\tMean reward: 0.13373399587820342 --- Total Reward: 41.59127271812127 --- EXP-EXP: 0.01\n",
      "Epoch 1690\n",
      "\tMean reward: 0.38508277581068706 --- Total Reward: 119.76074327712368 --- EXP-EXP: 0.01\n",
      "Epoch 1700\n",
      "\tMean reward: 0.5862007440664082 --- Total Reward: 182.30843140465296 --- EXP-EXP: 0.01\n",
      "Epoch 1710\n",
      "\tMean reward: 0.3465011435822783 --- Total Reward: 107.76185565408855 --- EXP-EXP: 0.01\n",
      "Epoch 1720\n",
      "\tMean reward: 0.13140282543196577 --- Total Reward: 40.86627870934135 --- EXP-EXP: 0.01\n",
      "Epoch 1730\n",
      "\tMean reward: 0.14782880067581525 --- Total Reward: 45.97475701017854 --- EXP-EXP: 0.01\n",
      "Epoch 1740\n",
      "\tMean reward: 0.11453817358735117 --- Total Reward: 35.621371985666215 --- EXP-EXP: 0.01\n",
      "Epoch 1750\n",
      "\tMean reward: 0.11204822640636006 --- Total Reward: 34.84699841237798 --- EXP-EXP: 0.01\n",
      "Epoch 1760\n",
      "\tMean reward: 0.4516645578545658 --- Total Reward: 140.46767749276995 --- EXP-EXP: 0.01\n",
      "Epoch 1770\n",
      "\tMean reward: 0.13197940205135533 --- Total Reward: 41.04559403797151 --- EXP-EXP: 0.01\n",
      "Epoch 1780\n",
      "\tMean reward: 0.3936507894223075 --- Total Reward: 122.42539551033764 --- EXP-EXP: 0.01\n",
      "Epoch 1790\n",
      "\tMean reward: 0.26049238525674445 --- Total Reward: 81.01313181484753 --- EXP-EXP: 0.01\n",
      "Epoch 1800\n",
      "\tMean reward: 0.43285331178020414 --- Total Reward: 134.6173799636435 --- EXP-EXP: 0.01\n",
      "Epoch 1810\n",
      "\tMean reward: 0.022837167261399293 --- Total Reward: 7.10235901829518 --- EXP-EXP: 0.01\n",
      "Epoch 1820\n",
      "\tMean reward: 0.6437594055512323 --- Total Reward: 200.20917512643325 --- EXP-EXP: 0.01\n",
      "Epoch 1830\n",
      "\tMean reward: 0.05961124583281217 --- Total Reward: 18.539097454004583 --- EXP-EXP: 0.01\n",
      "Epoch 1840\n",
      "\tMean reward: 0.41877564502345455 --- Total Reward: 130.23922560229437 --- EXP-EXP: 0.01\n",
      "Epoch 1850\n",
      "\tMean reward: 0.1265703464272552 --- Total Reward: 39.363377738876366 --- EXP-EXP: 0.01\n",
      "Epoch 1860\n",
      "\tMean reward: 0.38270091048957466 --- Total Reward: 119.01998316225772 --- EXP-EXP: 0.01\n",
      "Epoch 1870\n",
      "\tMean reward: 0.04458319521422772 --- Total Reward: 13.86537371162482 --- EXP-EXP: 0.01\n",
      "Epoch 1880\n",
      "\tMean reward: 0.48675791032938176 --- Total Reward: 151.38171011243773 --- EXP-EXP: 0.01\n",
      "Epoch 1890\n",
      "\tMean reward: 0.06173846834733518 --- Total Reward: 19.20066365602124 --- EXP-EXP: 0.01\n",
      "Epoch 1900\n",
      "\tMean reward: 0.2922748508593502 --- Total Reward: 90.8974786172579 --- EXP-EXP: 0.01\n",
      "Epoch 1910\n",
      "\tMean reward: 0.17610369753793542 --- Total Reward: 54.76824993429791 --- EXP-EXP: 0.01\n",
      "Epoch 1920\n",
      "\tMean reward: 0.08679222595812269 --- Total Reward: 26.992382272976158 --- EXP-EXP: 0.01\n",
      "Epoch 1930\n",
      "\tMean reward: 0.3519842156079636 --- Total Reward: 109.46709105407669 --- EXP-EXP: 0.01\n",
      "Epoch 1940\n",
      "\tMean reward: 0.08384664763188074 --- Total Reward: 26.076307413514908 --- EXP-EXP: 0.01\n",
      "Epoch 1950\n",
      "\tMean reward: 0.4559477861624744 --- Total Reward: 141.79976149652953 --- EXP-EXP: 0.01\n",
      "Epoch 1960\n",
      "\tMean reward: 0.50997223390001 --- Total Reward: 158.60136474290312 --- EXP-EXP: 0.01\n",
      "Epoch 1970\n",
      "\tMean reward: 0.055691945851330533 --- Total Reward: 17.320195159763795 --- EXP-EXP: 0.01\n",
      "Epoch 1980\n",
      "\tMean reward: 0.17416578081247114 --- Total Reward: 54.16555783267852 --- EXP-EXP: 0.01\n",
      "Epoch 1990\n",
      "\tMean reward: 0.5300198312140655 --- Total Reward: 164.83616750757437 --- EXP-EXP: 0.01\n",
      "Epoch 2000\n",
      "\tMean reward: 0.2024373888194997 --- Total Reward: 62.9580279228644 --- EXP-EXP: 0.01\n",
      "Epoch 2010\n",
      "\tMean reward: 0.15992461547531067 --- Total Reward: 49.73655541282162 --- EXP-EXP: 0.01\n",
      "Epoch 2020\n",
      "\tMean reward: 0.2342893770882848 --- Total Reward: 72.86399627445657 --- EXP-EXP: 0.01\n",
      "Epoch 2030\n",
      "\tMean reward: 0.27965782302627334 --- Total Reward: 86.973582961171 --- EXP-EXP: 0.01\n",
      "Epoch 2040\n",
      "\tMean reward: 0.10793672028232906 --- Total Reward: 33.56832000780434 --- EXP-EXP: 0.01\n",
      "Epoch 2050\n",
      "\tMean reward: 0.6008050623613074 --- Total Reward: 186.85037439436658 --- EXP-EXP: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-36f27c34b42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcurrent_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mglobal_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfertig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-fb99ed562e34>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_series_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1113\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Setup work for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0mtraining_distributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \"\"\"\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m       \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mset_value\u001b[0;34m(x, value)\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2847\u001b[0;31m       \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "    global_steps = 0\n",
    "    stats = {\"loss\": [], \"acc\": [], \"rew\": []}\n",
    "    for epoch in range(epochs):\n",
    "        state, info = simulation.reset()\n",
    "        # print(info)\n",
    "        current_rewards = []\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            global_steps += 1\n",
    "            reward, fertig, new_state = simulation.make_action(action)\n",
    "            current_rewards.append(reward)\n",
    "            agent.remember(state, action, reward, new_state, fertig)\n",
    "            \n",
    "            if global_steps % n_step == 0:\n",
    "                history = agent.replay()\n",
    "                if history:\n",
    "                    curr_loss = history[\"loss\"][0]\n",
    "                    curr_acc = history[\"acc\"][0]\n",
    "                    stats[\"loss\"].append(curr_loss)\n",
    "                    stats[\"acc\"].append(curr_acc)\n",
    "                \n",
    "            if global_steps % update_target_network == 0:\n",
    "                agent.target_train()\n",
    "    \n",
    "            state = new_state\n",
    "    \n",
    "            if fertig:\n",
    "                history = agent.replay()\n",
    "                curr_loss = history[\"loss\"][0]\n",
    "                curr_acc = history[\"acc\"][0]\n",
    "                curr_rew = np.sum(current_rewards)\n",
    "                curr_mean_rew = np.mean(current_rewards)\n",
    "                agent.sess.run(\n",
    "                    [\n",
    "                        agent.reward.assign(curr_rew), \n",
    "                        agent.reward_mean.assign(curr_mean_rew), \n",
    "                        agent.loss.assign(curr_loss), \n",
    "                        agent.accuracy.assign(curr_acc)\n",
    "                    ]\n",
    "                )\n",
    "                summary = agent.sess.run(agent.merged)\n",
    "                agent.writer.add_summary(summary, epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"Epoch {}\".format(epoch))\n",
    "                    print(\n",
    "                        \"\\tMean reward: {} --- Total Reward: {} --- EXP-EXP: {}\".format(curr_mean_rew, curr_rew, agent.epsilon)\n",
    "                    )\n",
    "                    agent.save()\n",
    "                break\n",
    "    agent.writer.close()\n",
    "    agent.sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
