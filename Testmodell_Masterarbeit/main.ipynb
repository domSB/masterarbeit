{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masterarbeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhaltsverzeichnis\n",
    "\n",
    "1. Importstatements\n",
    "2. Datenvorbereitung\n",
    "3. Simulationsmodell\n",
    "4. Q-Learning-Agent\n",
    "5. Hyperparameters\n",
    "6. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import summary, Variable, Session, name_scope\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wetterdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weather(path, start, ende):\n",
    "    df = pd.read_csv(\n",
    "        path, \n",
    "        index_col=\"date\", \n",
    "        memory_map=True\n",
    "\n",
    "        )\n",
    "    df = df.drop(columns=[\"Unnamed: 0\", \"HauptGruppe\", \"NebenGruppe\"])\n",
    "    # df = df.sort_index()\n",
    "    # df[\"Datum\"] = df.index.get_values()\n",
    "    # df[\"Datum\"] = pd.to_datetime(df[\"Datum\"]*24*3600, unit='s')\n",
    "    # df = df[df.Datum.dt.year.isin([2018,2019])]\n",
    "    # df = df[df.Datum.dt.dayofweek != 6]\n",
    "    df = df[df.index.isin(range(start, ende +3))]\n",
    "    # Plus 2 Tage, da Wetter von morgen und übermorgen\n",
    "    return df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preisdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prices(path):\n",
    "    df = pd.read_csv(\n",
    "        path, \n",
    "        names=[\"Zeile\", \"Preis\",\"Artikelnummer\",\"Datum\"],\n",
    "        header=0,\n",
    "        index_col=\"Artikelnummer\", \n",
    "        memory_map=True\n",
    "        )\n",
    "    df = df.sort_index()\n",
    "    df = df.drop(columns=[\"Zeile\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absatzdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sales(path):\n",
    "    # TODO: Statische Artikelinfo aus der Absatztabelle rausnehmen. (Warengruppe, Abteilung)\n",
    "    \"\"\"\n",
    "     for artikel in train_data[\"Artikel\"].unique():\n",
    "         warengruppen.append([artikel, train_data.loc[(slice(None), slice(5550,5550)),:].iloc[0].Warengruppe])\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path, \n",
    "        names=[\"Zeile\", \"Datum\", \"Artikel\", \"Absatz\", \"Warengruppe\", \"Abteilung\"], \n",
    "        header=0, \n",
    "        parse_dates=[1], \n",
    "        index_col=[1, 2],\n",
    "        memory_map=True\n",
    "        )\n",
    "    df.dropna(how='any', inplace=True)\n",
    "    df[\"Warengruppe\"] = df[\"Warengruppe\"].astype(np.uint8)\n",
    "    df = df.drop(columns=['Abteilung', 'Zeile'])\n",
    "    # Warengruppen auswählen\n",
    "    # 13 Frischmilch\n",
    "    # 14 Joghurt\n",
    "    # 69 Tabak\n",
    "    # 8 Obst Allgemen\n",
    "    # warengruppen = [8, 13, 14, 69 ]\n",
    "    warengruppen = [8]\n",
    "    df = df[df['Warengruppe'].isin(warengruppen)]\n",
    "    for i, wg in enumerate(warengruppen):\n",
    "        df.loc[df.Warengruppe == wg, \"Warengruppe\"] = i\n",
    "    df[\"Datum\"] = df.index.get_level_values('Datum')\n",
    "    df[\"Artikel\"] = df.index.get_level_values('Artikel').astype(np.int32)\n",
    "    # df[\"Wochentag\"] = df[\"Datum\"].apply(lambda x:x.dayofweek)\n",
    "    # df[\"Jahrestag\"] = df[\"Datum\"].apply(lambda x:x.dayofyear)\n",
    "    df[\"UNIXTag\"] = df[\"Datum\"].astype(np.int64)/(1000000000 * 24 * 3600)\n",
    "    df[\"Jahr\"] = df[\"Datum\"].apply(lambda x:x.year)\n",
    "    # df = df.drop(columns=['Datum'])\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    test_data = df[df[\"Jahr\"]==2019]\n",
    "    train_data = df[df[\"Jahr\"]==2018]\n",
    "    return test_data, train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data_to_numpy(big_df, artikel, start, end):\n",
    "    \"\"\"Returns a numpy array with lenght = self.kalendertage. Days without Sales are filled with zeros\"\"\"\n",
    "    s = big_df[big_df.Artikel == artikel].copy()\n",
    "    s.set_index(s.UNIXTag, inplace=True)\n",
    "    wg = s.iloc[0][[\"Warengruppe\"]][0]\n",
    "    s = s.drop(columns=[\"Datum\", \"Artikel\", \"Warengruppe\", \"Jahr\", \"UNIXTag\"])\n",
    "    s = s.reindex(range(int(start), int(end+1)), fill_value=0)\n",
    "\n",
    "    return s.to_numpy(), wg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulationsmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockSimulation:\n",
    "    def __init__(self, data_dir, time_series_lenght):\n",
    "        \"\"\"\n",
    "        Lädt Daten selbstständig aus Data_dir und erstellt das Simulationsmodell. \n",
    "        1. Episode entspricht einem Durchlauf mit einem Artikel.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        test_data, train_data = load_sales(os.path.join(data_dir, '3 absatz_altforweiler.csv'))\n",
    "\n",
    "        self.df = train_data\n",
    "\n",
    "        self.start_tag = int(min(train_data[\"UNIXTag\"]))\n",
    "        self.end_tag = int(max(train_data[\"UNIXTag\"]))\n",
    "        self.kalender_tage = self.end_tag - self.start_tag + 1\n",
    "\n",
    "        preise = load_prices(os.path.join(data_dir, '3 preise_altforweiler.csv'))\n",
    "\n",
    "        self.wetter = load_weather(os.path.join(data_dir, '2 wetter_saarlouis.csv'), self.start_tag, self.end_tag)\n",
    "        \n",
    "        self.warengruppen = self.df[\"Warengruppe\"].unique()\n",
    "        self.anz_wg = len(self.warengruppen)\n",
    "\n",
    "        self.anfangsbestand = np.random.randint(0,10)\n",
    "\n",
    "        self.time_series_lenght = time_series_lenght\n",
    "\n",
    "        olt = 1  # Fürs erste\n",
    "        self.fertig = None\n",
    "        self.vergangene_tage = None\n",
    "        self.akt_prod_bestand = None\n",
    "        self.akt_prod_absatz = None\n",
    "        self.akt_prod_wg = None\n",
    "        self.akt_prod_preis = None\n",
    "        self.akt_prod_olt = None\n",
    "        self.time_series_state = None\n",
    "\n",
    "        self.absatz_data = {}\n",
    "        self.static_state_data = {}\n",
    "        for artikel in tqdm(self.df[\"Artikel\"].unique()):\n",
    "            art_df, wg = copy_data_to_numpy(self.df, artikel, self.start_tag, self.end_tag)\n",
    "            self.absatz_data[artikel] = art_df\n",
    "            wg = to_categorical(wg, num_classes=self.anz_wg)\n",
    "\n",
    "            artikel_preis = preise.loc[artikel]\n",
    "\n",
    "            if type(artikel_preis) == pd.core.frame.DataFrame:\n",
    "                artikel_preis = np.array(\n",
    "                    [artikel_preis[artikel_preis.Datum == max(artikel_preis.Datum)][\"Preis\"].iat[0]]\n",
    "                )\n",
    "            elif type(artikel_preis) == pd.core.series.Series:\n",
    "                artikel_preis = np.array([artikel_preis[\"Preis\"]])\n",
    "            elif type(artikel_preis) == int:\n",
    "                artikel_preis = np.array([artikel_preis])\n",
    "            else:\n",
    "                raise AssertionError(\"Unknown Type for Price: {}\".format(type(artikel_preis)))\n",
    "            self.static_state_data[artikel] = {\"Warengruppe\":wg, \"OrderLeadTime\": olt, \"Preis\": artikel_preis}\n",
    "\n",
    "        self.aktueller_tag = self.start_tag\n",
    "        self.aktuelles_produkt = self.df[\"Artikel\"].sample(1).to_numpy()[0]\n",
    "\n",
    "    def create_new_state(self, wochentag):\n",
    "        new_state = np.concatenate(\n",
    "            [\n",
    "                np.array([self.akt_prod_bestand]), \n",
    "                wochentag, \n",
    "                self.akt_prod_wg, \n",
    "                self.akt_prod_preis, \n",
    "                self.wetter[self.vergangene_tage], \n",
    "                self.wetter[self.vergangene_tage+1]\n",
    "                ]\n",
    "            )\n",
    "        return new_state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        self.fertig = False\n",
    "        self.anfangsbestand = np.random.randint(0, 10)\n",
    "        self.aktueller_tag = self.start_tag\n",
    "        self.vergangene_tage = 0\n",
    "        self.aktuelles_produkt = self.df[\"Artikel\"].sample(1).to_numpy()[0]\n",
    "        self.akt_prod_bestand = self.anfangsbestand\n",
    "        self.akt_prod_absatz = self.absatz_data[self.aktuelles_produkt]\n",
    "        self.akt_prod_wg = self.static_state_data[self.aktuelles_produkt][\"Warengruppe\"]\n",
    "        self.akt_prod_preis = self.static_state_data[self.aktuelles_produkt][\"Preis\"]\n",
    "        self.akt_prod_olt = self.static_state_data[self.aktuelles_produkt][\"OrderLeadTime\"]\n",
    "\n",
    "        wochentag = self.aktueller_tag % 7\n",
    "\n",
    "        wochentag = to_categorical(wochentag, num_classes=7)\n",
    "\n",
    "        new_state = self.create_new_state(wochentag)\n",
    "        \n",
    "        self.time_series_state = deque(maxlen=self.time_series_lenght)\n",
    "        for _ in range(self.time_series_lenght):\n",
    "            self.time_series_state.append(new_state)\n",
    "        return np.array(self.time_series_state), {\"Artikel\": self.aktuelles_produkt}\n",
    "\n",
    "    def make_action(self, action):\n",
    "        if self.fertig:\n",
    "            raise AssertionError(\"Simulation für diesen Artikel fertig. Simulation zurücksetzen\")\n",
    "\n",
    "        absatz = self.akt_prod_absatz[self.vergangene_tage][0]\n",
    "\n",
    "        self.aktueller_tag += 1\n",
    "        self.vergangene_tage += 1\n",
    "\n",
    "        if self.aktueller_tag % 7 == 3: # Sonntag\n",
    "            self.aktueller_tag += 1\n",
    "            self.vergangene_tage += 1\n",
    "        \n",
    "        wochentag = self.aktueller_tag % 7\n",
    "\n",
    "        # Action ist die Bestellte Menge an Artikeln\n",
    "        # Tagsüber Absatz abziehen:\n",
    "        self.akt_prod_bestand -= absatz\n",
    "\n",
    "        # Nachmittag: Bestellung kommt an\n",
    "        self.akt_prod_bestand += action\n",
    "\n",
    "        # Abend: Bestand wird bewertet\n",
    "        if self.akt_prod_bestand >= 1:\n",
    "            reward = np.exp((-self.akt_prod_bestand+1)/5)\n",
    "        else:\n",
    "            reward = np.exp((self.akt_prod_bestand-1)*1.5-1)\n",
    "            # Nichtnegativität des Bestandes\n",
    "            self.akt_prod_bestand = 0\n",
    "\n",
    "        wochentag = to_categorical(wochentag, num_classes=7)\n",
    "        \n",
    "        new_state = self.create_new_state(wochentag)\n",
    "\n",
    "        self.time_series_state.append(new_state)\n",
    "\n",
    "        if self.vergangene_tage == self.kalender_tage -1:\n",
    "            self.fertig = True\n",
    "        \n",
    "        return reward, self.fertig, np.array(self.time_series_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, \n",
    "                 memory_size, \n",
    "                 state_shape, \n",
    "                 action_space, \n",
    "                 gamma, \n",
    "                 learning_rate, \n",
    "                 batch_size, \n",
    "                 epsilon, \n",
    "                 epsilon_decay, \n",
    "                 epsilon_min, \n",
    "                 possible_actions, \n",
    "                 time_series_length\n",
    "                 ):\n",
    "        self.memory_size = memory_size\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon \n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.possible_actions = possible_actions\n",
    "        self.time_series_length = time_series_length\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.model = self.create_model(\"Train\")\n",
    "        self.logdir = \"./logs/\" + datetime.datetime.today().date().__str__() + \"-\" \\\n",
    "                      + datetime.datetime.today().time().__str__()[:8].replace(\":\", \".\")\n",
    "        self.target_model = self.create_model(\"Target\")\n",
    "        self.sess = Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        self.writer = summary.FileWriter(self.logdir, self.sess.graph)\n",
    "        with tf.name_scope(\"Eigene_Variablen\"):\n",
    "            with tf.device('/gpu:0'):\n",
    "                self.reward = Variable(0.0, trainable=False, name=\"vReward\")\n",
    "                self.reward_mean = Variable(0.0, trainable=False, name=\"vMeanReward\")\n",
    "                self.loss = Variable(0.0, trainable=False, name=\"vLoss\")\n",
    "                self.accuracy = Variable(0.0, trainable=False, name=\"vMSE\")\n",
    "        self.summary_reward = summary.scalar(\"Reward\", self.reward)\n",
    "        self.summary_reward_mean = summary.scalar(\"MeanReward\", self.reward_mean)\n",
    "        self.summary_loss = summary.scalar(\"Loss\", self.loss)\n",
    "        self.summary_mse = summary.scalar(\"Accuracy\", self.accuracy)\n",
    "        self.merged = summary.merge(\n",
    "            [\n",
    "                self.summary_reward, \n",
    "                self.summary_reward_mean, \n",
    "                self.summary_loss, \n",
    "                self.summary_mse\n",
    "            ])\n",
    "\n",
    "    def create_model(self, name):\n",
    "        with name_scope(name):\n",
    "            inputs = Input(shape=(self.time_series_length, self.state_shape))\n",
    "            x = LSTM(32, activation='relu', name=\"LSTM\")(inputs)\n",
    "            x = Dense(32, activation='relu', name=\"Dense_1\")(x)\n",
    "            x = Dense(64, activation='relu', name=\"Dense_2\")(x)\n",
    "            predictions = Dense(self.action_space, activation='relu', name=\"Predictions\")(x)\n",
    "            model = Model(inputs=inputs, outputs=predictions)\n",
    "            model.compile(optimizer=RMSprop(lr=self.learning_rate), loss='mse', metrics=[\"accuracy\"])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = [sample[0] for sample in samples]\n",
    "        actions = [sample[1] for sample in samples]\n",
    "        rewards = [sample[2] for sample in samples]\n",
    "        new_states = [sample[3] for sample in samples]\n",
    "        new_states = np.array(new_states)\n",
    "        states = np.array(states)\n",
    "        dones = [sample[4] for sample in samples]\n",
    "        targets = self.target_model.predict(states)\n",
    "        qs_new_states = self.target_model.predict(new_states)\n",
    "        \n",
    "        target_qs_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            terminal = dones[i]\n",
    "\n",
    "            if terminal:\n",
    "                updated_target = targets[i]\n",
    "                updated_target[actions[i]] = rewards[i]\n",
    "                target_qs_batch.append(updated_target)\n",
    "            else:\n",
    "                updated_target = targets[i]\n",
    "                updated_target[actions[i]] = rewards[i] + self.gamma * np.max(qs_new_states[i])\n",
    "                target_qs_batch.append(updated_target)\n",
    "\n",
    "        targets = np.array([each for each in target_qs_batch])\n",
    "        with tf.device('/gpu:0'):\n",
    "            history = self.model.fit(states, targets, epochs=1, verbose=0, callbacks=[])\n",
    "        return history.history\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i]\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = np.max([self.epsilon, self.epsilon_min])\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.sample(self.possible_actions, 1)[0]\n",
    "        with tf.device('/gpu:0'):\n",
    "            predictions = self.model.predict(state.reshape(1, self.time_series_length, self.state_shape))[0]\n",
    "        return np.argmax(predictions)\n",
    "    \n",
    "    def save(self):\n",
    "        agent.target_model.save(\"model/model.h5\")\n",
    "    \n",
    "    def load(self):\n",
    "        model = load_model(\"model/model.h5\")\n",
    "        agent.target_model = model\n",
    "        agent.model = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = True\n",
    "\n",
    "use_saved_model = False\n",
    "\n",
    "\n",
    "memory_size = 364*500\n",
    "gamma = 0.5\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99999\n",
    "learning_rate = 0.001\n",
    "tau = 0.05\n",
    "batch_size = 128\n",
    "n_step = 300\n",
    "log_frequency = 100  # jeder 100te n_step\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "update_target_network = batch_size * 100\n",
    "\n",
    "state_shape = 24\n",
    "action_space = 10\n",
    "\n",
    "time_series_length = 10\n",
    "\n",
    "order_none = 0\n",
    "order_one = 1\n",
    "order_two = 2\n",
    "order_tree = 3\n",
    "order_four = 4\n",
    "order_five = 5\n",
    "order_six = 6\n",
    "order_seven = 7\n",
    "order_eight = 8\n",
    "order_nine = 9\n",
    "\n",
    "possible_actions = [\n",
    "    order_none, \n",
    "    order_one, \n",
    "    order_two, \n",
    "    order_tree, \n",
    "    order_four, \n",
    "    order_five, \n",
    "    order_six, \n",
    "    order_seven, \n",
    "    order_eight, \n",
    "    order_nine\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsloop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.environ[\"OS\"] == \"Windows_NT\"\n",
    "    # Bin am eigenen Desktop\n",
    "    data_dir = 'F:/OneDrive/Dokumente/1 Universität - Master/6. Semester/Masterarbeit/Implementation/Echtdaten'\n",
    "except KeyError:\n",
    "    # Bin auf der EC2 Linux Maschine \n",
    "    data_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "100%|██████████| 566/566 [00:02<00:00, 194.37it/s]\n"
     ]
    }
   ],
   "source": [
    "simulation = StockSimulation(data_dir, time_series_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "agent = DQN(\n",
    "    memory_size, \n",
    "    state_shape, \n",
    "    action_space, \n",
    "    gamma,\n",
    "    learning_rate, \n",
    "    batch_size, \n",
    "    epsilon, \n",
    "    epsilon_decay, \n",
    "    epsilon_min, \n",
    "    possible_actions, \n",
    "    time_series_length\n",
    "    )\n",
    "\n",
    "if use_saved_model:\n",
    "    agent.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 0\n",
      "\tMean reward: 0.04171865395717855 --- Total Reward: 12.974501380682531 --- EXP-EXP: 0.9968948155387197\n",
      "Epoch 10\n",
      "\tMean reward: 0.007923153687144795 --- Total Reward: 2.464100796702031 --- EXP-EXP: 0.9663683806346631\n",
      "Epoch 20\n",
      "\tMean reward: 0.03232289919921837 --- Total Reward: 10.052421650956914 --- EXP-EXP: 0.9367767115789445\n"
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "    global_steps = 0\n",
    "    stats = {\"loss\": [], \"acc\": [], \"rew\": []}\n",
    "    for epoch in range(epochs):\n",
    "        state, info = simulation.reset()\n",
    "        # print(info)\n",
    "        current_rewards = []\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            global_steps += 1\n",
    "            reward, fertig, new_state = simulation.make_action(action)\n",
    "            current_rewards.append(reward)\n",
    "            agent.remember(state, action, reward, new_state, fertig)\n",
    "            \n",
    "            if global_steps % n_step == 0:\n",
    "                history = agent.replay()\n",
    "                if history:\n",
    "                    curr_loss = history[\"loss\"][0]\n",
    "                    curr_acc = history[\"acc\"][0]\n",
    "                    stats[\"loss\"].append(curr_loss)\n",
    "                    stats[\"acc\"].append(curr_acc)\n",
    "                \n",
    "            if global_steps % update_target_network == 0:\n",
    "                agent.target_train()\n",
    "    \n",
    "            state = new_state\n",
    "    \n",
    "            if fertig:\n",
    "                history = agent.replay()\n",
    "                curr_loss = history[\"loss\"][0]\n",
    "                curr_acc = history[\"acc\"][0]\n",
    "                curr_rew = np.sum(current_rewards)\n",
    "                curr_mean_rew = np.mean(current_rewards)\n",
    "                agent.sess.run(\n",
    "                    [\n",
    "                        agent.reward.assign(curr_rew), \n",
    "                        agent.reward_mean.assign(curr_mean_rew), \n",
    "                        agent.loss.assign(curr_loss), \n",
    "                        agent.accuracy.assign(curr_acc)\n",
    "                    ]\n",
    "                )\n",
    "                tf_summary = agent.sess.run(agent.merged, options=run_options, run_metadata=run_metadata)\n",
    "                agent.writer.add_summary(tf_summary, epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"Epoch {}\".format(epoch))\n",
    "                    print(\n",
    "                        \"\\tMean reward: {} --- Total Reward: {} --- EXP-EXP: {}\".format(curr_mean_rew, curr_rew, agent.epsilon)\n",
    "                    )\n",
    "                    agent.save()\n",
    "                break\n",
    "    agent.writer.close()\n",
    "    agent.sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
